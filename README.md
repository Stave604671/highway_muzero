# highway_muzero


# 目前遇到的问题

1、当前使用的观测空间和连续型动作空间的特点，就是训练过程中车辆会很容易失控出轨，导致前期震荡很严重，
模型训练过程中基本学不到什么知识，或者学习很慢，导致收敛速度和效果很差。

2、因为上面这个原因，导致total reward曲线震荡。

3、似乎离散型动作空间的逻辑可以缓解这个过程，而且离散型动作空间下reward更合理，mean reward的上升也正常些。
而且离散型动作空间的逻辑更加高级，连续型动作空间的代码更久一些，可以把两边代码一起升级一下。

# 核心实验步骤【为避免浪费训练时长】

1、涉及到对比实验，或者对比效果的情况，需要给每个配置文件单独写一个py文件，并且在文件名体现核心

2、每生成一个对比图，将对应的截图保存提交到代码中，在readme记录结论，避免重复实验


# 已知的训练结论
1、学习率在最合适的数值下，reward才可以稳定收敛。学习率下降虽然会促进模型本身更稳定收敛，reward也会更平滑，
但是最终其实会因为reward收敛速度下降，导致训练周期被拉的更长。
2、环境刷新频率和策略刷新频率就用目前的24/12就好，改别的话，本身使用这个连续动作空间，动作空间存在角度，导致车辆就不稳定，
如果策略更新太快，会导致车辆状态更新频率更容易出轨。
3、如果动作空间的加速度一直处于负数，会导致车辆速度越来越慢，然后速度也成为负数，导致车辆开始倒退，所以代码里面加了强行控制，让车辆继续开！
4、resnet网络虽然更强，但是推理速度太慢不符合要求，而且训练初期使用全连接网络，更容易对其他参数进行调整。
        """
        例子：
        假设在一个自动驾驶任务中，车辆需要计划如何通过一个复杂的交通路口。设置 td_steps 为 5 意味着模型将根据未来的 
        5 个状态及其奖励来估计当前决策的价值。这可以帮助模型更好地权衡在复杂路口中短期和长期的风险与收益。
        总结：
        td_steps 是一个控制时间差分更新步数的参数，用于决定在计算当前状态的目标价值时，要向未来看多少步。
        它影响了模型在短期与长期回报之间的权衡，配置合适的 td_steps 对于提升模型的表现至关重要。
        """
        self.td_steps = 50 
5、上面这个参数，给高了，容易被历史数据影响现在推理，给低了，收敛幅度过小，给适当的数值合适。
而且，似乎和这个数值一起配合一个更适当的self.num_unroll_steps收敛效果更好。
6、全连接下self.entropy_loss_weight给0.10更理想一些，给太低了会导致后期reward下降，给高了趋势前期就容易冒峰值，不利于稳定？？？？  todo？？不过好像还可以高一些

7、